#!/usr/bin/python

# Copyright (c) 2011 Recoset <nicolas@recoset.com>
# 
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
# 
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
# 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.


import urllib, urllib2, urlparse

from NagAconda import Plugin


class EmptyData(Exception):
    pass


def get_data(url):
    usock = urllib2.urlopen(url)
    data = usock.read()
    usock.close()
    if not data:
        raise EmptyData
    return data


def parse_data(data):
    # Parse "target,start_timestamp,end_timestamp,step|value1,...,valueN"
    # into metric/target name and value list
    pieces = data.split("|")
    counter = pieces[0].split(",")[0]
    values = pieces[1].strip().split(",")
    return counter, values


def prev_period_url(url):
        parts = urlparse.urlparse(url)
        params = dict(urlparse.parse_qsl(parts.query))
        params['from'] = "-2minutes"
        params['until'] = "-1minutes"
        parts = list(parts)
        parts[4] = urllib.urlencode(params)
        new_url = urlparse.urlunparse(parts)
        return new_url


# Initialization
graphite = Plugin("Plugin to retrieve data from graphite", "1.0")
graphite.add_option("u", "url", "URL to query for data", required=True)
graphite.enable_status("warning")
graphite.enable_status("critical")
graphite.start()

try:
    # Get data from Graphite URL API
    data = get_data(graphite.options.url)

    # Parse it into metric name, values
    name, values = parse_data(data)

    # Handle situation where we're looking for a single value and it is None.
    # This most often occurs when we queried Graphite between timeperiod
    # rollover and deposit of data.
    # It assumes one is querying via eg &from=-1minutes.
    if len(values) == 1 and values[0] == 'None':
        # Single retry looking at previous time period instead of requested one
        # -- which should have its data filled in fine.
        new_url = prev_period_url(graphite.options.url)
        data = get_data(new_url)
        name, values = parse_data(data)
    values = map(float, values)

    # Calculate an average from all values, that's our metric
    avg = sum(values)/len(values);
    graphite.set_value(name, avg)

    # Report and finish
    graphite.set_status_message("Avg value of %s was %f" % (name, avg))
    graphite.finish()
# Handle case where we got empty data back -- eg bad metric path / 500 / etc
except EmptyData:
    graphite.unknown_error("Server returned no data! Double check your metric path?")
# Handle any other exceptions to correctly return UNKNOWN
except Exception, e:
    graphite.unknown_error("Python error: %s" % e)
